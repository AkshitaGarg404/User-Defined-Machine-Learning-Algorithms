{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMVlz1e63tbxd/qk6uQD15W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GTcnGe7MzJfU","executionInfo":{"status":"ok","timestamp":1728486130368,"user_tz":-330,"elapsed":4071,"user":{"displayName":"AKSHITA GARG","userId":"12410317472217733424"}},"outputId":"cb28a2ab-6a60-4432-b361-3eb3d33e41f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 1, 1, 1, 2, 1, 0, 0, 0, 2, 1, 1, 0, 1, 2, 2, 1, 1, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 2, 2]\n","Analysing our model.\n","[[11.  0.  0.]\n"," [ 0. 10.  2.]\n"," [ 0.  0.  7.]]\n","Accuracy: [1.         0.93333333 0.93333333]\n","Precision: [1.         1.         0.77777778]\n","Recall: [1.         0.83333333 1.        ]\n","F1 Score: [1.         0.90909091 0.875     ]\n"]}],"source":["import numpy as np\n","import sklearn as sk\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split  #inbuilt data split tool\n","\n","def covmat(data):\n","  num_features=data.shape[1] #get Number of features\n","  num_sample=len(data)  #get Number of samples\n","  cov_matrix = np.zeros((num_features, num_features)) #create cov matrix\n","  mean=np.sum(data,axis=0)/num_sample #mean for all collumns\n","  for i in range (num_features):\n","    for j in range (num_features):\n","      cov=0;\n","      for k in range (num_sample): #for each sample take sum\n","        cov+=((data[k,i]-mean[i])*(data[k,j]-mean[j]))/(num_sample-1)\n","        cov_matrix[i,j]=cov\n","  return cov_matrix\n","\n","def mahalanobis(x_test,x_train,y_train):\n","  num_sample=x_train.shape[0] #number of training samples\n","\n","  unique_classes, class_counts = np.unique(y_train, return_counts=True) #returns all unique classes with their counts\n","  class_mean_inv = {} #to store mean and inverse of all classes\n","  for c in unique_classes:\n","    class_indices = np.where(y_train == c)[0] #returns list of all indices of class c\n","    class_data = x_train[class_indices] #get all train data of class c\n","    mean = np.mean(class_data, axis=0) #find mean of that class\n","    cov_matrix = covmat(class_data) #find cov matrix of that class\n","    inverse_covmat = np.linalg.inv(cov_matrix)  #inverse of covariance matrix of that class\n","    class_mean_inv[c] = {'mean': mean, 'inverse_covmat': inverse_covmat}\n","\n","  predicted=[] #to store predicted classes\n","  #To store predictions\n","  for test_sample in x_test:\n","    a=[] #to store all distances\n","    for c in unique_classes:\n","      mean = class_mean_inv[c]['mean']\n","      inverse_covmat = class_mean_inv[c]['inverse_covmat']\n","      diff=test_sample-mean #(x-meani)\n","      dist=np.sqrt(np.dot(diff,np.dot(inverse_covmat,diff.T))) #|(x-meani)cov^-1(x-meani)^T|\n","      a.append(dist)\n","    predicted_class=np.argmin(a)\n","    predicted.append(predicted_class)\n","  return predicted\n","\n","def test_train_split(data,target,split_perc):\n","  perm=np.random.permutation(len(data)) #to get random permutation of 0 to len-1\n","  x_shuff=data[perm]    #obtain shuffled data\n","  y_shuff=target[perm]    #obtain shuffled targets\n","  #get the index till which data is in train set\n","  n=round((((100-split_perc)*0.01))*len(data)) #round is used to round off to the nearest integer\n","  x_train=x_shuff[:n] #take starting n elements of shuffled data\n","  y_train=y_shuff[:n]\n","  x_test=x_shuff[n:] #take remaining elements of shuffled data\n","  y_test=y_shuff[n:]\n","  return x_train, y_train, x_test, y_test\n","\n","iris=load_iris()\n","x_train,y_train,x_test,y_test=test_train_split(iris.data,iris.target,20)\n","predicted=mahalanobis(x_test, x_train, y_train)\n","print(predicted)\n","\n","#Analysis of this result using 3 class confusion matrix\n","print(\"Analysing our model.\")\n","\n","def calculate_confusion_matrix(y_true, y_pred, num_classes):\n","  # Initialize the confusion matrix of size(nXn) where n=number of classes\n","  conf_matrix = np.zeros((num_classes, num_classes))\n","  # Iterate over each sample to create confusion matrix with respect to A\n","  #.     A.   B.   C.   Predicted\n","  #.  A  TP   FN   FN\n","  #   B  FP   TN   TN\n","  #   C  FP   TN   TN\n","  for true_class, pred_class in zip(y_true, y_pred):\n","    #Update the corresponding cell in the confusion matrix\n","    conf_matrix[int(true_class), int(pred_class)] += 1\n","  return conf_matrix\n","def calculate_metrics(conf_matrix):\n","  #Calculating true positives, false positives, false negatives for each class\n","  TP = np.diag(conf_matrix)  #obtained as a matrix of order 1xn\n","  FP = np.sum(conf_matrix, axis=0) - TP   #sum of column-TP\n","  FN = np.sum(conf_matrix, axis=1) - TP   #sum of row -TP\n","  # calculating true negatives for each class\n","  TN = np.sum(conf_matrix) - (TP + FP + FN) #remaining part\n","\n","  # calculating accuracy for each class\n","  accuracy = (TP + TN) / np.sum(conf_matrix)\n","\n","  # calculating precision for each class\n","  precision = TP / (TP + FP)\n","\n","  # calculating recall for each class\n","  recall = TP / (TP + FN)\n","\n","  # calculating F1 score for each class\n","  f1_score = 2 * (precision * recall) / (precision + recall)\n","\n","  return accuracy, precision, recall, f1_score\n","\n","\n","conf_matrix=calculate_confusion_matrix(y_test,predicted,3)\n","accuracy, precision, recall, f1_score = calculate_metrics(conf_matrix)\n","print(conf_matrix)\n","print(\"Accuracy:\", accuracy)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"F1 Score:\", f1_score)"]}]}